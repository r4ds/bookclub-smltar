[["index.html", "Supervised Machine Learning for Text Analysis in R Book Club Welcome", " Supervised Machine Learning for Text Analysis in R Book Club The R4DS Online Learning Community 2022-02-05 Welcome Welcome to the bookclub! This is a companion for the book Supervised Machine Learning for Text Analysis in R by Emil Hvitfeldt and Julia Silge (Chapman and Hall/CRC, copyright 2021, 9781003093459). This companion is available at r4ds.io/smltar. This website is being developed by the R4DS Online Learning Community. Follow along, and join the community to participate. This companion follows the R4DS Online Learning Community Code of Conduct. "],["book-club-meetings.html", "Book club meetings", " Book club meetings Each week, a volunteer will present a chapter from the book (or part of a chapter). This is the best way to learn the material. Presentations will usually consist of a review of the material, a discussion, and/or a demonstration of the principles presented in that chapter. More information about how to present is available in the github repo. Presentations will be recorded, and will be available on the R4DS Online Learning Community YouTube Channel. "],["pace.html", "Pace", " Pace We’ll try to cover 1 chapter/week, but… …It’s ok to split chapters when they feel like too much. We will try to meet every week, but will likely take some breaks for holidays, etc. "],["language-and-modeling.html", "Chapter 1 Language and modeling", " Chapter 1 Language and modeling Learning objectives: Understand subfields of linguistics. Describe how morphology plays a role in text modeling. Understand the limitation of different languages. Understand how text can vary. "],["linguistics-for-text-analysis.html", "1.1 Linguistics for Text Analysis", " 1.1 Linguistics for Text Analysis Understanding this hierarchy will help create natural language features. For example: - using a text that has been broken into sequences of characters for recurrent neural network - morphology - utilizing the part of speech information as features - syntax Most linguists view speech as primary to written language (technological). Analyzing written text can be limiting as it is less “creative” and more abstract. "],["a-glimpse-into-morphology.html", "1.2 A Glimpse into Morphology", " 1.2 A Glimpse into Morphology The study of words, their internal structures and how they are formed “source: http://designpublic.in/blogs/morphological-awareness-underrated-contributor-reading-ability/” English has a pretty low ratio of morphemes (smallest unit of part of a word with meaning) eg “un-”, “break”, “-able” Understanding morphological characteristics are beneficial for pre-processing, removing stopwords, and end stemming "],["different-languages.html", "1.3 Different Languages", " 1.3 Different Languages Remember, English is NOT the only language BenderRule : acknowledge that the models being built are typically language-specific. - neglecting to state the language may give a false veneer of language-independence to the work Thusly, most text used for modeling henceforth will be in English. "],["other-ways-text-can-vary.html", "1.4 Other Ways Text can Vary", " 1.4 Other Ways Text can Vary Dialects (e.e AAVE &amp; detecting hate speech) Evolution of language Usage of slang Like ML in general, text modeling is very sensitive to the data used for training! "],["meeting-videos.html", "1.5 Meeting Videos", " 1.5 Meeting Videos 1.5.1 Cohort 1 Meeting chat log (somewhat ironically, there was no text log for this meeting) "],["tokenization.html", "Chapter 2 Tokenization", " Chapter 2 Tokenization Learning objectives: Define “Token” Different types of tokens Where does tokenization break down? Building your own tokenizer "],["define-token.html", "2.1 Define “Token”", " 2.1 Define “Token” In tokenization, we take an input text, and break it up into pieces of meaningful sizes. We refer this pieces of texts as tokens. Most commonly, input texts are broken up into words. But this isn’t perfect. No white space in certain languages (她是我最好的朋友。- ‘she is my best friend’ in Mandarin) Pronouns/negation words (Je n’aime pas le chocolat - ‘I don’t like chocolate’ in French) Contractions of two words (would’ve, didn’t) Knowing this, let’s take a jab at tokenization. We’ll split on anything that is not an alphanumeric character. library(tidyverse) library(hcandersenr) the_fir_tree &lt;- hcandersen_en %&gt;% filter(book == &quot;The fir tree&quot;) %&gt;% pull(text) head(the_fir_tree, 9) ## [1] &quot;Far down in the forest, where the warm sun and the fresh air made a sweet&quot; ## [2] &quot;resting-place, grew a pretty little fir-tree; and yet it was not happy, it&quot; ## [3] &quot;wished so much to be tall like its companions– the pines and firs which grew&quot; ## [4] &quot;around it. The sun shone, and the soft air fluttered its leaves, and the&quot; ## [5] &quot;little peasant children passed by, prattling merrily, but the fir-tree heeded&quot; ## [6] &quot;them not. Sometimes the children would bring a large basket of raspberries or&quot; ## [7] &quot;strawberries, wreathed on a straw, and seat themselves near the fir-tree, and&quot; ## [8] &quot;say, \\&quot;Is it not a pretty little tree?\\&quot; which made it feel more unhappy than&quot; ## [9] &quot;before.&quot; strsplit(the_fir_tree[1:2], &quot;[^a-zA-Z0-9]+&quot;) ## [[1]] ## [1] &quot;Far&quot; &quot;down&quot; &quot;in&quot; &quot;the&quot; &quot;forest&quot; &quot;where&quot; &quot;the&quot; &quot;warm&quot; ## [9] &quot;sun&quot; &quot;and&quot; &quot;the&quot; &quot;fresh&quot; &quot;air&quot; &quot;made&quot; &quot;a&quot; &quot;sweet&quot; ## ## [[2]] ## [1] &quot;resting&quot; &quot;place&quot; &quot;grew&quot; &quot;a&quot; &quot;pretty&quot; &quot;little&quot; &quot;fir&quot; ## [8] &quot;tree&quot; &quot;and&quot; &quot;yet&quot; &quot;it&quot; &quot;was&quot; &quot;not&quot; &quot;happy&quot; ## [15] &quot;it&quot; This is pretty good, but the hero’s name (fir-tree) has been split. This kind of information loss can be dangerous, and we need to use more delicate splitting methods, rather than brute-forcing with regex. Luckily, you don’t have to write all the custom logics yourself! This chapter introduces tokenizers library(tokenizers) tokenize_words(the_fir_tree[1:2]) ## [[1]] ## [1] &quot;far&quot; &quot;down&quot; &quot;in&quot; &quot;the&quot; &quot;forest&quot; &quot;where&quot; &quot;the&quot; &quot;warm&quot; ## [9] &quot;sun&quot; &quot;and&quot; &quot;the&quot; &quot;fresh&quot; &quot;air&quot; &quot;made&quot; &quot;a&quot; &quot;sweet&quot; ## ## [[2]] ## [1] &quot;resting&quot; &quot;place&quot; &quot;grew&quot; &quot;a&quot; &quot;pretty&quot; &quot;little&quot; &quot;fir&quot; ## [8] &quot;tree&quot; &quot;and&quot; &quot;yet&quot; &quot;it&quot; &quot;was&quot; &quot;not&quot; &quot;happy&quot; ## [15] &quot;it&quot; The word-level tokenization in this package is done by finding word boundaries, which follows a number of sophisticated rules. "],["different-types-of-tokens.html", "2.2 Different types of tokens", " 2.2 Different types of tokens We can tokenize texts at a variety of units including: characters words sentences lines paragraphs n-grams Let’s explore how tokenizers handle these. sample_vector &lt;- c(&quot;Far down in the forest&quot;, &quot;grew a pretty little fir-tree&quot;) sample_tibble &lt;- tibble(text = sample_vector) tokenize_words(sample_vector) ## [[1]] ## [1] &quot;far&quot; &quot;down&quot; &quot;in&quot; &quot;the&quot; &quot;forest&quot; ## ## [[2]] ## [1] &quot;grew&quot; &quot;a&quot; &quot;pretty&quot; &quot;little&quot; &quot;fir&quot; &quot;tree&quot; tidytext::unnest_tokens does the same thing, but in a different data structure. Under the hood, it uses paste0(\"tokenize_\", token) from the tokenizers package. library(tidytext) sample_tibble %&gt;% unnest_tokens(word, text, token = &quot;words&quot;) ## # A tibble: 11 × 1 ## word ## &lt;chr&gt; ## 1 far ## 2 down ## 3 in ## 4 the ## 5 forest ## 6 grew ## 7 a ## 8 pretty ## 9 little ## 10 fir ## 11 tree You can pass in arguments used in tokenize_words() through unnest_tokens() using … sample_tibble %&gt;% unnest_tokens(word, text, token = &quot;words&quot;, strip_punct = FALSE) ## # A tibble: 12 × 1 ## word ## &lt;chr&gt; ## 1 far ## 2 down ## 3 in ## 4 the ## 5 forest ## 6 grew ## 7 a ## 8 pretty ## 9 little ## 10 fir ## 11 - ## 12 tree 2.2.1 Token type: Character tokenize_characters() splits the text into letters. If strip_non_alphanum is TRUE, it strips all [:punct:] &amp; [:whitespace:] before doing the the word boundaries. tft_token_characters &lt;- tokenize_characters(x = the_fir_tree, lowercase = TRUE, strip_non_alphanum = TRUE, simplify = FALSE) head(tft_token_characters) %&gt;% glimpse() ## List of 6 ## $ : chr [1:57] &quot;f&quot; &quot;a&quot; &quot;r&quot; &quot;d&quot; ... ## $ : chr [1:57] &quot;r&quot; &quot;e&quot; &quot;s&quot; &quot;t&quot; ... ## $ : chr [1:61] &quot;w&quot; &quot;i&quot; &quot;s&quot; &quot;h&quot; ... ## $ : chr [1:56] &quot;a&quot; &quot;r&quot; &quot;o&quot; &quot;u&quot; ... ## $ : chr [1:64] &quot;l&quot; &quot;i&quot; &quot;t&quot; &quot;t&quot; ... ## $ : chr [1:64] &quot;t&quot; &quot;h&quot; &quot;e&quot; &quot;m&quot; ... The same thing in unnest_tokens() tibble(text = the_fir_tree) %&gt;% unnest_tokens(word, text, token = &quot;characters&quot;) ## # A tibble: 13,429 × 1 ## word ## &lt;chr&gt; ## 1 f ## 2 a ## 3 r ## 4 d ## 5 o ## 6 w ## 7 n ## 8 i ## 9 n ## 10 t ## # … with 13,419 more rows Watch out for ligatures! Ligatures are when multiple letters are combined as a single character. tokenize_characters(&quot;straße&quot;) ## [[1]] ## [1] &quot;s&quot; &quot;t&quot; &quot;r&quot; &quot;a&quot; &quot;ß&quot; &quot;e&quot; Is it a meaningful feature to keep? Is is sylistic or functional? “Wie trinken die Schweizer Bier? – In Massen.” (“How do the Swiss drink beer? – In mass” instead of other meaning if it had been written as “in Maßen”: “in moderation”) 2.2.2 Token type: Word We’ve already seen this before, but chaining this with dplyr can result in interesting analyses. hcandersen_en %&gt;% filter(book %in% c(&quot;The fir tree&quot;, &quot;The little mermaid&quot;)) %&gt;% unnest_tokens(word, text) %&gt;% count(book, word) %&gt;% group_by(book) %&gt;% arrange(desc(n)) %&gt;% slice(1:5) ## # A tibble: 10 × 3 ## # Groups: book [2] ## book word n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 The fir tree the 278 ## 2 The fir tree and 161 ## 3 The fir tree tree 76 ## 4 The fir tree it 66 ## 5 The fir tree a 56 ## 6 The little mermaid the 817 ## 7 The little mermaid and 398 ## 8 The little mermaid of 252 ## 9 The little mermaid she 240 ## 10 The little mermaid to 199 2.2.3 Token type: n-grams A continuous sequence of n items. (syllables, letters, words, …) unigram: “Hello,” “day,” “my,” “little” bigram: “fir tree,” “fresh air,” “to be,” “Robin Hood” trigram: “You and I,” “please let go,” “no time like,” “the little mermaid” n-grams can capture meaningful word orders that can otherwise be lost. (“fir tree”) It does so, by sliding across the text, to create overlapping sets of tokens. tft_token_ngram &lt;- tokenize_ngrams(x = the_fir_tree, lowercase = TRUE, n = 3L, n_min = 3L, stopwords = character(), ngram_delim = &quot; &quot;, simplify = FALSE) tft_token_ngram[[1]] ## [1] &quot;far down in&quot; &quot;down in the&quot; &quot;in the forest&quot; &quot;the forest where&quot; ## [5] &quot;forest where the&quot; &quot;where the warm&quot; &quot;the warm sun&quot; &quot;warm sun and&quot; ## [9] &quot;sun and the&quot; &quot;and the fresh&quot; &quot;the fresh air&quot; &quot;fresh air made&quot; ## [13] &quot;air made a&quot; &quot;made a sweet&quot; Using unigram is fast, but lose some information. Using higher n-grams keeps more information, but token counts decrease. You have to balance this trade off. You can of course, tokenize many n-grams in the same place, by using n &amp; n_min parameters. tft_token_ngram &lt;- tokenize_ngrams(x = the_fir_tree, n = 2L, n_min = 1L) tft_token_ngram[[1]] ## [1] &quot;far&quot; &quot;far down&quot; &quot;down&quot; &quot;down in&quot; &quot;in&quot; ## [6] &quot;in the&quot; &quot;the&quot; &quot;the forest&quot; &quot;forest&quot; &quot;forest where&quot; ## [11] &quot;where&quot; &quot;where the&quot; &quot;the&quot; &quot;the warm&quot; &quot;warm&quot; ## [16] &quot;warm sun&quot; &quot;sun&quot; &quot;sun and&quot; &quot;and&quot; &quot;and the&quot; ## [21] &quot;the&quot; &quot;the fresh&quot; &quot;fresh&quot; &quot;fresh air&quot; &quot;air&quot; ## [26] &quot;air made&quot; &quot;made&quot; &quot;made a&quot; &quot;a&quot; &quot;a sweet&quot; ## [31] &quot;sweet&quot; This is beneficial, because unigrams would capture the frequency of the words, and the bigrams would supplement the meaning of tokens, that unigrams didn’t catch. 2.2.4 Token type: Lines, sentence, and paragraph These rather large token types are rarely used for modelling purposes. The common approach for these, is to collapse all strings into one giant string, and split them using a delimeter. 2.2.4.1 Chapters/paragraphs add_paragraphs &lt;- function(data) { pull(data, text) %&gt;% paste(collapse = &quot;\\n&quot;) %&gt;% tokenize_paragraphs() %&gt;% unlist() %&gt;% tibble(text = .) %&gt;% mutate(paragraph = row_number()) } library(janeaustenr) northangerabbey_paragraphed &lt;- tibble(text = northangerabbey) %&gt;% mutate(chapter = cumsum(str_detect(text, &quot;^CHAPTER &quot;))) %&gt;% filter(chapter &gt; 0, !str_detect(text, &quot;^CHAPTER &quot;)) %&gt;% nest(data = text) %&gt;% mutate(data = map(data, add_paragraphs)) %&gt;% unnest(cols = c(data)) glimpse(northangerabbey_paragraphed) ## Rows: 1,020 ## Columns: 3 ## $ chapter &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, … ## $ text &lt;chr&gt; &quot;No one who had ever seen Catherine Morland in her infancy w… ## $ paragraph &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1… 2.2.4.2 Sentences Convert the fir tree from “one line per element” to “one line per sentence”. the_fir_tree_sentences &lt;- the_fir_tree %&gt;% paste(collapse = &quot; &quot;) %&gt;% tokenize_sentences() head(the_fir_tree_sentences[[1]]) ## [1] &quot;Far down in the forest, where the warm sun and the fresh air made a sweet resting-place, grew a pretty little fir-tree; and yet it was not happy, it wished so much to be tall like its companions– the pines and firs which grew around it.&quot; ## [2] &quot;The sun shone, and the soft air fluttered its leaves, and the little peasant children passed by, prattling merrily, but the fir-tree heeded them not.&quot; ## [3] &quot;Sometimes the children would bring a large basket of raspberries or strawberries, wreathed on a straw, and seat themselves near the fir-tree, and say, \\&quot;Is it not a pretty little tree?\\&quot;&quot; ## [4] &quot;which made it feel more unhappy than before.&quot; ## [5] &quot;And yet all this while the tree grew a notch or joint taller every year; for by the number of joints in the stem of a fir-tree we can discover its age.&quot; ## [6] &quot;Still, as it grew, it complained.&quot; "],["where-does-tokenization-break-down.html", "2.3 Where does tokenization break down?", " 2.3 Where does tokenization break down? Tokenization is a crucial first step to any kind of text analysis. Defaults work well for the most part, but we do have to make decisions carefully. Don’t forget you owe the bank $1 million for the house. Don’t: 1 word? or “do” &amp; “n’t”? $1 &amp; .: strip_punct? Context matters. On Twitter, you’ll run into grammatically incorrect sentences with multiple spaces, deliberate capitalization, different styles, … You may not be worried, if you’re just interested in what words are used. However, you may need to be more careful, if you’re doing a social grouping analysis. Another thing to consider is the degree of compression &amp; speed each tokenizing methods provide. You don’t want to choose a method that gives fewer tokens, just because it’s faster. You may lose some information. "],["building-your-own-tokenizer.html", "2.4 Building your own tokenizer", " 2.4 Building your own tokenizer Regex time! There are two approaches Split the string up according to some rule. Extract tokens based on some rule. 2.4.1 Mimick tokenize_characters() From a string, we can extract the letters one by one. letter_tokens &lt;- str_extract_all( string = &quot;This sentence include 2 numbers and 1 period.&quot;, pattern = &quot;[:alpha:]{1}&quot; ) letter_tokens ## [[1]] ## [1] &quot;T&quot; &quot;h&quot; &quot;i&quot; &quot;s&quot; &quot;s&quot; &quot;e&quot; &quot;n&quot; &quot;t&quot; &quot;e&quot; &quot;n&quot; &quot;c&quot; &quot;e&quot; &quot;i&quot; &quot;n&quot; &quot;c&quot; &quot;l&quot; &quot;u&quot; &quot;d&quot; &quot;e&quot; ## [20] &quot;n&quot; &quot;u&quot; &quot;m&quot; &quot;b&quot; &quot;e&quot; &quot;r&quot; &quot;s&quot; &quot;a&quot; &quot;n&quot; &quot;d&quot; &quot;p&quot; &quot;e&quot; &quot;r&quot; &quot;i&quot; &quot;o&quot; &quot;d&quot; We have to be careful what we put in the regex danish_sentence &lt;- &quot;Så mødte han en gammel heks på landevejen&quot; str_extract_all(danish_sentence, &quot;[:alpha:]&quot;) ## [[1]] ## [1] &quot;S&quot; &quot;å&quot; &quot;m&quot; &quot;ø&quot; &quot;d&quot; &quot;t&quot; &quot;e&quot; &quot;h&quot; &quot;a&quot; &quot;n&quot; &quot;e&quot; &quot;n&quot; &quot;g&quot; &quot;a&quot; &quot;m&quot; &quot;m&quot; &quot;e&quot; &quot;l&quot; &quot;h&quot; ## [20] &quot;e&quot; &quot;k&quot; &quot;s&quot; &quot;p&quot; &quot;å&quot; &quot;l&quot; &quot;a&quot; &quot;n&quot; &quot;d&quot; &quot;e&quot; &quot;v&quot; &quot;e&quot; &quot;j&quot; &quot;e&quot; &quot;n&quot; str_extract_all(danish_sentence, &quot;[a-zA-Z]&quot;) ## [[1]] ## [1] &quot;S&quot; &quot;m&quot; &quot;d&quot; &quot;t&quot; &quot;e&quot; &quot;h&quot; &quot;a&quot; &quot;n&quot; &quot;e&quot; &quot;n&quot; &quot;g&quot; &quot;a&quot; &quot;m&quot; &quot;m&quot; &quot;e&quot; &quot;l&quot; &quot;h&quot; &quot;e&quot; &quot;k&quot; ## [20] &quot;s&quot; &quot;p&quot; &quot;l&quot; &quot;a&quot; &quot;n&quot; &quot;d&quot; &quot;e&quot; &quot;v&quot; &quot;e&quot; &quot;j&quot; &quot;e&quot; &quot;n&quot; 2.4.2 Allow for hyphenated words in tokenize_words() Let’s make “fir-tree” a single word token. One way to do this is to split texts on white space, and dropping punctuations. str_split(&quot;This isn&#39;t a sentence with fir-tree.&quot;, &quot;[:space:]&quot;) ## [[1]] ## [1] &quot;This&quot; &quot;isn&#39;t&quot; &quot;a&quot; &quot;sentence&quot; &quot;with&quot; &quot;fir-tree.&quot; str_split(&quot;This isn&#39;t a sentence with fir-tree.&quot;, &quot;[:space:]&quot;) %&gt;% map(~ str_remove_all(.x, &quot;^[:punct:]+|[:punct:]+$&quot;)) ## [[1]] ## [1] &quot;This&quot; &quot;isn&#39;t&quot; &quot;a&quot; &quot;sentence&quot; &quot;with&quot; &quot;fir-tree&quot; Another way is to extract the hyphenated word. str_extract_all( string = &quot;This isn&#39;t a sentence with fir-tree.&quot;, pattern = &quot;[:alpha:]+-[:alpha:]+&quot; ) ## [[1]] ## [1] &quot;fir-tree&quot; use ? quantifier in regex, to optionally match pattern. str_extract_all( string = &quot;This isn&#39;t a sentence with fir-tree.&quot;, pattern = &quot;[:alpha:]+-?[:alpha:]+&quot; ) ## [[1]] ## [1] &quot;This&quot; &quot;isn&quot; &quot;sentence&quot; &quot;with&quot; &quot;fir-tree&quot; include ' in the [:alpha:] class str_extract_all( string = &quot;This isn&#39;t a sentence with fir-tree.&quot;, pattern = &quot;[[:alpha:]&#39;]+-?[[:alpha:]&#39;]+&quot; ) ## [[1]] ## [1] &quot;This&quot; &quot;isn&#39;t&quot; &quot;sentence&quot; &quot;with&quot; &quot;fir-tree&quot; The letter “a” is missing, because the regex so far assumes at least 2 characters. Get around that, by using | to set up a match for one or more [:alpha:] str_extract_all( string = &quot;This isn&#39;t a sentence with fir-tree.&quot;, pattern = &quot;[[:alpha:]&#39;]+-?[[:alpha:]&#39;]+|[:alpha:]{1}&quot; ) ## [[1]] ## [1] &quot;This&quot; &quot;isn&#39;t&quot; &quot;a&quot; &quot;sentence&quot; &quot;with&quot; &quot;fir-tree&quot; 2.4.3 Character n-gram tokenizer tokenize_character_ngram &lt;- function(x, n) { ngram_loc &lt;- str_locate_all(x, paste0(&quot;(?=(\\\\w{&quot;, n, &quot;}))&quot;)) map2(ngram_loc, x, ~str_sub(.y, .x[, 1], .x[, 1] + n - 1)) } tokenize_character_ngram(the_fir_tree[1:3], n = 3) ## [[1]] ## [1] &quot;Far&quot; &quot;dow&quot; &quot;own&quot; &quot;the&quot; &quot;for&quot; &quot;ore&quot; &quot;res&quot; &quot;est&quot; &quot;whe&quot; &quot;her&quot; &quot;ere&quot; &quot;the&quot; ## [13] &quot;war&quot; &quot;arm&quot; &quot;sun&quot; &quot;and&quot; &quot;the&quot; &quot;fre&quot; &quot;res&quot; &quot;esh&quot; &quot;air&quot; &quot;mad&quot; &quot;ade&quot; &quot;swe&quot; ## [25] &quot;wee&quot; &quot;eet&quot; ## ## [[2]] ## [1] &quot;res&quot; &quot;est&quot; &quot;sti&quot; &quot;tin&quot; &quot;ing&quot; &quot;pla&quot; &quot;lac&quot; &quot;ace&quot; &quot;gre&quot; &quot;rew&quot; &quot;pre&quot; &quot;ret&quot; ## [13] &quot;ett&quot; &quot;tty&quot; &quot;lit&quot; &quot;itt&quot; &quot;ttl&quot; &quot;tle&quot; &quot;fir&quot; &quot;tre&quot; &quot;ree&quot; &quot;and&quot; &quot;yet&quot; &quot;was&quot; ## [25] &quot;not&quot; &quot;hap&quot; &quot;app&quot; &quot;ppy&quot; ## ## [[3]] ## [1] &quot;wis&quot; &quot;ish&quot; &quot;she&quot; &quot;hed&quot; &quot;muc&quot; &quot;uch&quot; &quot;tal&quot; &quot;all&quot; &quot;lik&quot; &quot;ike&quot; &quot;its&quot; &quot;com&quot; ## [13] &quot;omp&quot; &quot;mpa&quot; &quot;pan&quot; &quot;ani&quot; &quot;nio&quot; &quot;ion&quot; &quot;ons&quot; &quot;the&quot; &quot;pin&quot; &quot;ine&quot; &quot;nes&quot; &quot;and&quot; ## [25] &quot;fir&quot; &quot;irs&quot; &quot;whi&quot; &quot;hic&quot; &quot;ich&quot; &quot;gre&quot; &quot;rew&quot; "],["meeting-videos-1.html", "2.5 Meeting Videos", " 2.5 Meeting Videos 2.5.1 Cohort 1 Meeting chat log 00:41:34 shamsuddeen: https://github.com/quanteda/spacyr "],["stop-words.html", "Chapter 3 Stop words", " Chapter 3 Stop words Learning objectives: (these are optional but helpful) "],["slide-1.html", "3.1 Slide 1", " 3.1 Slide 1 Add slides as sections (marked with ##). Please give code chunks unique names (such as “01-something” for a block in chapter 1). This makes debugging much easier. "],["meeting-videos-2.html", "3.2 Meeting Videos", " 3.2 Meeting Videos 3.2.1 Cohort 1 Meeting chat log LOG "],["stemming.html", "Chapter 4 Stemming", " Chapter 4 Stemming Learning objectives: (these are optional but helpful) "],["slide-1-1.html", "4.1 Slide 1", " 4.1 Slide 1 Add slides as sections (marked with ##). Please give code chunks unique names (such as “01-something” for a block in chapter 1). This makes debugging much easier. "],["meeting-videos-3.html", "4.2 Meeting Videos", " 4.2 Meeting Videos 4.2.1 Cohort 1 Meeting chat log LOG "],["word-embeddings.html", "Chapter 5 Word Embeddings", " Chapter 5 Word Embeddings Learning objectives: (these are optional but helpful) "],["slide-1-2.html", "5.1 Slide 1", " 5.1 Slide 1 Add slides as sections (marked with ##). Please give code chunks unique names (such as “01-something” for a block in chapter 1). This makes debugging much easier. "],["meeting-videos-4.html", "5.2 Meeting Videos", " 5.2 Meeting Videos 5.2.1 Cohort 1 Meeting chat log LOG "],["regression.html", "Chapter 6 Regression", " Chapter 6 Regression Learning objectives: (these are optional but helpful) "],["slide-1-3.html", "6.1 Slide 1", " 6.1 Slide 1 Add slides as sections (marked with ##). Please give code chunks unique names (such as “01-something” for a block in chapter 1). This makes debugging much easier. "],["meeting-videos-5.html", "6.2 Meeting Videos", " 6.2 Meeting Videos 6.2.1 Cohort 1 Meeting chat log LOG "],["classification.html", "Chapter 7 Classification", " Chapter 7 Classification Learning objectives: (these are optional but helpful) "],["slide-1-4.html", "7.1 Slide 1", " 7.1 Slide 1 Add slides as sections (marked with ##). Please give code chunks unique names (such as “01-something” for a block in chapter 1). This makes debugging much easier. "],["meeting-videos-6.html", "7.2 Meeting Videos", " 7.2 Meeting Videos 7.2.1 Cohort 1 Meeting chat log LOG "],["dense-neural-networks.html", "Chapter 8 Dense neural networks", " Chapter 8 Dense neural networks Learning objectives: (these are optional but helpful) "],["slide-1-5.html", "8.1 Slide 1", " 8.1 Slide 1 Add slides as sections (marked with ##). Please give code chunks unique names (such as “01-something” for a block in chapter 1). This makes debugging much easier. "],["meeting-videos-7.html", "8.2 Meeting Videos", " 8.2 Meeting Videos 8.2.1 Cohort 1 Meeting chat log LOG "],["long-short-term-memory-lstm-networks.html", "Chapter 9 Long short-term memory (LSTM) networks", " Chapter 9 Long short-term memory (LSTM) networks Learning objectives: (these are optional but helpful) "],["slide-1-6.html", "9.1 Slide 1", " 9.1 Slide 1 Add slides as sections (marked with ##). Please give code chunks unique names (such as “01-something” for a block in chapter 1). This makes debugging much easier. "],["meeting-videos-8.html", "9.2 Meeting Videos", " 9.2 Meeting Videos 9.2.1 Cohort 1 Meeting chat log LOG "],["convolutional-neural-networks.html", "Chapter 10 Convolutional neural networks", " Chapter 10 Convolutional neural networks Learning objectives: (these are optional but helpful) "],["slide-1-7.html", "10.1 Slide 1", " 10.1 Slide 1 Add slides as sections (marked with ##). Please give code chunks unique names (such as “01-something” for a block in chapter 1). This makes debugging much easier. "],["meeting-videos-9.html", "10.2 Meeting Videos", " 10.2 Meeting Videos 10.2.1 Cohort 1 Meeting chat log LOG "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
