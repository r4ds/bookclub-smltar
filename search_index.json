[["index.html", "Supervised Machine Learning for Text Analysis in R Book Club Welcome", " Supervised Machine Learning for Text Analysis in R Book Club The R4DS Online Learning Community 2022-03-10 Welcome Welcome to the bookclub! This is a companion for the book Supervised Machine Learning for Text Analysis in R by Emil Hvitfeldt and Julia Silge (Chapman and Hall/CRC, copyright 2021, 9781003093459). This companion is available at r4ds.io/smltar. This website is being developed by the R4DS Online Learning Community. Follow along, and join the community to participate. This companion follows the R4DS Online Learning Community Code of Conduct. "],["book-club-meetings.html", "Book club meetings", " Book club meetings Each week, a volunteer will present a chapter from the book (or part of a chapter). This is the best way to learn the material. Presentations will usually consist of a review of the material, a discussion, and/or a demonstration of the principles presented in that chapter. More information about how to present is available in the github repo. Presentations will be recorded, and will be available on the R4DS Online Learning Community YouTube Channel. "],["pace.html", "Pace", " Pace We’ll try to cover 1 chapter/week, but… …It’s ok to split chapters when they feel like too much. We will try to meet every week, but will likely take some breaks for holidays, etc. "],["language-and-modeling.html", "Chapter 1 Language and modeling", " Chapter 1 Language and modeling Learning objectives: Understand subfields of linguistics. Describe how morphology plays a role in text modeling. Understand the limitation of different languages. Understand how text can vary. "],["linguistics-for-text-analysis.html", "1.1 Linguistics for Text Analysis", " 1.1 Linguistics for Text Analysis Understanding this hierarchy will help create natural language features. For example: - using a text that has been broken into sequences of characters for recurrent neural network - morphology - utilizing the part of speech information as features - syntax Most linguists view speech as primary to written language (technological). Analyzing written text can be limiting as it is less “creative” and more abstract. "],["a-glimpse-into-morphology.html", "1.2 A Glimpse into Morphology", " 1.2 A Glimpse into Morphology The study of words, their internal structures and how they are formed “source: http://designpublic.in/blogs/morphological-awareness-underrated-contributor-reading-ability/” English has a pretty low ratio of morphemes (smallest unit of part of a word with meaning) eg “un-”, “break”, “-able” Understanding morphological characteristics are beneficial for pre-processing, removing stopwords, and end stemming "],["different-languages.html", "1.3 Different Languages", " 1.3 Different Languages Remember, English is NOT the only language BenderRule : acknowledge that the models being built are typically language-specific. - neglecting to state the language may give a false veneer of language-independence to the work Thusly, most text used for modeling henceforth will be in English. "],["other-ways-text-can-vary.html", "1.4 Other Ways Text can Vary", " 1.4 Other Ways Text can Vary Dialects (e.e AAVE &amp; detecting hate speech) Evolution of language Usage of slang Like ML in general, text modeling is very sensitive to the data used for training! "],["meeting-videos.html", "1.5 Meeting Videos", " 1.5 Meeting Videos 1.5.1 Cohort 1 Meeting chat log (somewhat ironically, there was no text log for this meeting) "],["tokenization.html", "Chapter 2 Tokenization", " Chapter 2 Tokenization Learning objectives: Define “Token” Different types of tokens Where does tokenization break down? Building your own tokenizer "],["define-token.html", "2.1 Define “Token”", " 2.1 Define “Token” In tokenization, we take an input text, and break it up into pieces of meaningful sizes. We refer this pieces of texts as tokens. Most commonly, input texts are broken up into words. But this isn’t perfect. No white space in certain languages (她是我最好的朋友。- ‘she is my best friend’ in Mandarin) Pronouns/negation words (Je n’aime pas le chocolat - ‘I don’t like chocolate’ in French) Contractions of two words (would’ve, didn’t) Knowing this, let’s take a jab at tokenization. We’ll split on anything that is not an alphanumeric character. library(tidyverse) library(hcandersenr) the_fir_tree &lt;- hcandersen_en %&gt;% filter(book == &quot;The fir tree&quot;) %&gt;% pull(text) head(the_fir_tree, 9) ## [1] &quot;Far down in the forest, where the warm sun and the fresh air made a sweet&quot; ## [2] &quot;resting-place, grew a pretty little fir-tree; and yet it was not happy, it&quot; ## [3] &quot;wished so much to be tall like its companions– the pines and firs which grew&quot; ## [4] &quot;around it. The sun shone, and the soft air fluttered its leaves, and the&quot; ## [5] &quot;little peasant children passed by, prattling merrily, but the fir-tree heeded&quot; ## [6] &quot;them not. Sometimes the children would bring a large basket of raspberries or&quot; ## [7] &quot;strawberries, wreathed on a straw, and seat themselves near the fir-tree, and&quot; ## [8] &quot;say, \\&quot;Is it not a pretty little tree?\\&quot; which made it feel more unhappy than&quot; ## [9] &quot;before.&quot; strsplit(the_fir_tree[1:2], &quot;[^a-zA-Z0-9]+&quot;) ## [[1]] ## [1] &quot;Far&quot; &quot;down&quot; &quot;in&quot; &quot;the&quot; &quot;forest&quot; &quot;where&quot; &quot;the&quot; &quot;warm&quot; ## [9] &quot;sun&quot; &quot;and&quot; &quot;the&quot; &quot;fresh&quot; &quot;air&quot; &quot;made&quot; &quot;a&quot; &quot;sweet&quot; ## ## [[2]] ## [1] &quot;resting&quot; &quot;place&quot; &quot;grew&quot; &quot;a&quot; &quot;pretty&quot; &quot;little&quot; &quot;fir&quot; ## [8] &quot;tree&quot; &quot;and&quot; &quot;yet&quot; &quot;it&quot; &quot;was&quot; &quot;not&quot; &quot;happy&quot; ## [15] &quot;it&quot; This is pretty good, but the hero’s name (fir-tree) has been split. This kind of information loss can be dangerous, and we need to use more delicate splitting methods, rather than brute-forcing with regex. Luckily, you don’t have to write all the custom logics yourself! This chapter introduces tokenizers library(tokenizers) tokenize_words(the_fir_tree[1:2]) ## [[1]] ## [1] &quot;far&quot; &quot;down&quot; &quot;in&quot; &quot;the&quot; &quot;forest&quot; &quot;where&quot; &quot;the&quot; &quot;warm&quot; ## [9] &quot;sun&quot; &quot;and&quot; &quot;the&quot; &quot;fresh&quot; &quot;air&quot; &quot;made&quot; &quot;a&quot; &quot;sweet&quot; ## ## [[2]] ## [1] &quot;resting&quot; &quot;place&quot; &quot;grew&quot; &quot;a&quot; &quot;pretty&quot; &quot;little&quot; &quot;fir&quot; ## [8] &quot;tree&quot; &quot;and&quot; &quot;yet&quot; &quot;it&quot; &quot;was&quot; &quot;not&quot; &quot;happy&quot; ## [15] &quot;it&quot; The word-level tokenization in this package is done by finding word boundaries, which follows a number of sophisticated rules. "],["different-types-of-tokens.html", "2.2 Different types of tokens", " 2.2 Different types of tokens We can tokenize texts at a variety of units including: characters words sentences lines paragraphs n-grams Let’s explore how tokenizers handle these. sample_vector &lt;- c(&quot;Far down in the forest&quot;, &quot;grew a pretty little fir-tree&quot;) sample_tibble &lt;- tibble(text = sample_vector) tokenize_words(sample_vector) ## [[1]] ## [1] &quot;far&quot; &quot;down&quot; &quot;in&quot; &quot;the&quot; &quot;forest&quot; ## ## [[2]] ## [1] &quot;grew&quot; &quot;a&quot; &quot;pretty&quot; &quot;little&quot; &quot;fir&quot; &quot;tree&quot; tidytext::unnest_tokens does the same thing, but in a different data structure. Under the hood, it uses paste0(\"tokenize_\", token) from the tokenizers package. library(tidytext) sample_tibble %&gt;% unnest_tokens(word, text, token = &quot;words&quot;) ## # A tibble: 11 × 1 ## word ## &lt;chr&gt; ## 1 far ## 2 down ## 3 in ## 4 the ## 5 forest ## 6 grew ## 7 a ## 8 pretty ## 9 little ## 10 fir ## 11 tree You can pass in arguments used in tokenize_words() through unnest_tokens() using … sample_tibble %&gt;% unnest_tokens(word, text, token = &quot;words&quot;, strip_punct = FALSE) ## # A tibble: 12 × 1 ## word ## &lt;chr&gt; ## 1 far ## 2 down ## 3 in ## 4 the ## 5 forest ## 6 grew ## 7 a ## 8 pretty ## 9 little ## 10 fir ## 11 - ## 12 tree 2.2.1 Token type: Character tokenize_characters() splits the text into letters. If strip_non_alphanum is TRUE, it strips all [:punct:] &amp; [:whitespace:] before doing the the word boundaries. tft_token_characters &lt;- tokenize_characters(x = the_fir_tree, lowercase = TRUE, strip_non_alphanum = TRUE, simplify = FALSE) head(tft_token_characters) %&gt;% glimpse() ## List of 6 ## $ : chr [1:57] &quot;f&quot; &quot;a&quot; &quot;r&quot; &quot;d&quot; ... ## $ : chr [1:57] &quot;r&quot; &quot;e&quot; &quot;s&quot; &quot;t&quot; ... ## $ : chr [1:61] &quot;w&quot; &quot;i&quot; &quot;s&quot; &quot;h&quot; ... ## $ : chr [1:56] &quot;a&quot; &quot;r&quot; &quot;o&quot; &quot;u&quot; ... ## $ : chr [1:64] &quot;l&quot; &quot;i&quot; &quot;t&quot; &quot;t&quot; ... ## $ : chr [1:64] &quot;t&quot; &quot;h&quot; &quot;e&quot; &quot;m&quot; ... The same thing in unnest_tokens() tibble(text = the_fir_tree) %&gt;% unnest_tokens(word, text, token = &quot;characters&quot;) ## # A tibble: 13,429 × 1 ## word ## &lt;chr&gt; ## 1 f ## 2 a ## 3 r ## 4 d ## 5 o ## 6 w ## 7 n ## 8 i ## 9 n ## 10 t ## # … with 13,419 more rows Watch out for ligatures! Ligatures are when multiple letters are combined as a single character. tokenize_characters(&quot;straße&quot;) ## [[1]] ## [1] &quot;s&quot; &quot;t&quot; &quot;r&quot; &quot;a&quot; &quot;ß&quot; &quot;e&quot; Is it a meaningful feature to keep? Is is sylistic or functional? “Wie trinken die Schweizer Bier? – In Massen.” (“How do the Swiss drink beer? – In mass” instead of other meaning if it had been written as “in Maßen”: “in moderation”) 2.2.2 Token type: Word We’ve already seen this before, but chaining this with dplyr can result in interesting analyses. hcandersen_en %&gt;% filter(book %in% c(&quot;The fir tree&quot;, &quot;The little mermaid&quot;)) %&gt;% unnest_tokens(word, text) %&gt;% count(book, word) %&gt;% group_by(book) %&gt;% arrange(desc(n)) %&gt;% slice(1:5) ## # A tibble: 10 × 3 ## # Groups: book [2] ## book word n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 The fir tree the 278 ## 2 The fir tree and 161 ## 3 The fir tree tree 76 ## 4 The fir tree it 66 ## 5 The fir tree a 56 ## 6 The little mermaid the 817 ## 7 The little mermaid and 398 ## 8 The little mermaid of 252 ## 9 The little mermaid she 240 ## 10 The little mermaid to 199 2.2.3 Token type: n-grams A continuous sequence of n items. (syllables, letters, words, …) unigram: “Hello,” “day,” “my,” “little” bigram: “fir tree,” “fresh air,” “to be,” “Robin Hood” trigram: “You and I,” “please let go,” “no time like,” “the little mermaid” n-grams can capture meaningful word orders that can otherwise be lost. (“fir tree”) It does so, by sliding across the text, to create overlapping sets of tokens. tft_token_ngram &lt;- tokenize_ngrams(x = the_fir_tree, lowercase = TRUE, n = 3L, n_min = 3L, stopwords = character(), ngram_delim = &quot; &quot;, simplify = FALSE) tft_token_ngram[[1]] ## [1] &quot;far down in&quot; &quot;down in the&quot; &quot;in the forest&quot; &quot;the forest where&quot; ## [5] &quot;forest where the&quot; &quot;where the warm&quot; &quot;the warm sun&quot; &quot;warm sun and&quot; ## [9] &quot;sun and the&quot; &quot;and the fresh&quot; &quot;the fresh air&quot; &quot;fresh air made&quot; ## [13] &quot;air made a&quot; &quot;made a sweet&quot; Using unigram is fast, but lose some information. Using higher n-grams keeps more information, but token counts decrease. You have to balance this trade off. You can of course, tokenize many n-grams in the same place, by using n &amp; n_min parameters. tft_token_ngram &lt;- tokenize_ngrams(x = the_fir_tree, n = 2L, n_min = 1L) tft_token_ngram[[1]] ## [1] &quot;far&quot; &quot;far down&quot; &quot;down&quot; &quot;down in&quot; &quot;in&quot; ## [6] &quot;in the&quot; &quot;the&quot; &quot;the forest&quot; &quot;forest&quot; &quot;forest where&quot; ## [11] &quot;where&quot; &quot;where the&quot; &quot;the&quot; &quot;the warm&quot; &quot;warm&quot; ## [16] &quot;warm sun&quot; &quot;sun&quot; &quot;sun and&quot; &quot;and&quot; &quot;and the&quot; ## [21] &quot;the&quot; &quot;the fresh&quot; &quot;fresh&quot; &quot;fresh air&quot; &quot;air&quot; ## [26] &quot;air made&quot; &quot;made&quot; &quot;made a&quot; &quot;a&quot; &quot;a sweet&quot; ## [31] &quot;sweet&quot; This is beneficial, because unigrams would capture the frequency of the words, and the bigrams would supplement the meaning of tokens, that unigrams didn’t catch. 2.2.4 Token type: Lines, sentence, and paragraph These rather large token types are rarely used for modelling purposes. The common approach for these, is to collapse all strings into one giant string, and split them using a delimeter. 2.2.4.1 Chapters/paragraphs add_paragraphs &lt;- function(data) { pull(data, text) %&gt;% paste(collapse = &quot;\\n&quot;) %&gt;% tokenize_paragraphs() %&gt;% unlist() %&gt;% tibble(text = .) %&gt;% mutate(paragraph = row_number()) } library(janeaustenr) northangerabbey_paragraphed &lt;- tibble(text = northangerabbey) %&gt;% mutate(chapter = cumsum(str_detect(text, &quot;^CHAPTER &quot;))) %&gt;% filter(chapter &gt; 0, !str_detect(text, &quot;^CHAPTER &quot;)) %&gt;% nest(data = text) %&gt;% mutate(data = map(data, add_paragraphs)) %&gt;% unnest(cols = c(data)) glimpse(northangerabbey_paragraphed) ## Rows: 1,020 ## Columns: 3 ## $ chapter &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, … ## $ text &lt;chr&gt; &quot;No one who had ever seen Catherine Morland in her infancy w… ## $ paragraph &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1… 2.2.4.2 Sentences Convert the fir tree from “one line per element” to “one line per sentence”. the_fir_tree_sentences &lt;- the_fir_tree %&gt;% paste(collapse = &quot; &quot;) %&gt;% tokenize_sentences() head(the_fir_tree_sentences[[1]]) ## [1] &quot;Far down in the forest, where the warm sun and the fresh air made a sweet resting-place, grew a pretty little fir-tree; and yet it was not happy, it wished so much to be tall like its companions– the pines and firs which grew around it.&quot; ## [2] &quot;The sun shone, and the soft air fluttered its leaves, and the little peasant children passed by, prattling merrily, but the fir-tree heeded them not.&quot; ## [3] &quot;Sometimes the children would bring a large basket of raspberries or strawberries, wreathed on a straw, and seat themselves near the fir-tree, and say, \\&quot;Is it not a pretty little tree?\\&quot;&quot; ## [4] &quot;which made it feel more unhappy than before.&quot; ## [5] &quot;And yet all this while the tree grew a notch or joint taller every year; for by the number of joints in the stem of a fir-tree we can discover its age.&quot; ## [6] &quot;Still, as it grew, it complained.&quot; "],["where-does-tokenization-break-down.html", "2.3 Where does tokenization break down?", " 2.3 Where does tokenization break down? Tokenization is a crucial first step to any kind of text analysis. Defaults work well for the most part, but we do have to make decisions carefully. Don’t forget you owe the bank $1 million for the house. Don’t: 1 word? or “do” &amp; “n’t”? $1 &amp; .: strip_punct? Context matters. On Twitter, you’ll run into grammatically incorrect sentences with multiple spaces, deliberate capitalization, different styles, … You may not be worried, if you’re just interested in what words are used. However, you may need to be more careful, if you’re doing a social grouping analysis. Another thing to consider is the degree of compression &amp; speed each tokenizing methods provide. You don’t want to choose a method that gives fewer tokens, just because it’s faster. You may lose some information. "],["building-your-own-tokenizer.html", "2.4 Building your own tokenizer", " 2.4 Building your own tokenizer Regex time! There are two approaches Split the string up according to some rule. Extract tokens based on some rule. 2.4.1 Mimick tokenize_characters() From a string, we can extract the letters one by one. letter_tokens &lt;- str_extract_all( string = &quot;This sentence include 2 numbers and 1 period.&quot;, pattern = &quot;[:alpha:]{1}&quot; ) letter_tokens ## [[1]] ## [1] &quot;T&quot; &quot;h&quot; &quot;i&quot; &quot;s&quot; &quot;s&quot; &quot;e&quot; &quot;n&quot; &quot;t&quot; &quot;e&quot; &quot;n&quot; &quot;c&quot; &quot;e&quot; &quot;i&quot; &quot;n&quot; &quot;c&quot; &quot;l&quot; &quot;u&quot; &quot;d&quot; &quot;e&quot; ## [20] &quot;n&quot; &quot;u&quot; &quot;m&quot; &quot;b&quot; &quot;e&quot; &quot;r&quot; &quot;s&quot; &quot;a&quot; &quot;n&quot; &quot;d&quot; &quot;p&quot; &quot;e&quot; &quot;r&quot; &quot;i&quot; &quot;o&quot; &quot;d&quot; We have to be careful what we put in the regex danish_sentence &lt;- &quot;Så mødte han en gammel heks på landevejen&quot; str_extract_all(danish_sentence, &quot;[:alpha:]&quot;) ## [[1]] ## [1] &quot;S&quot; &quot;å&quot; &quot;m&quot; &quot;ø&quot; &quot;d&quot; &quot;t&quot; &quot;e&quot; &quot;h&quot; &quot;a&quot; &quot;n&quot; &quot;e&quot; &quot;n&quot; &quot;g&quot; &quot;a&quot; &quot;m&quot; &quot;m&quot; &quot;e&quot; &quot;l&quot; &quot;h&quot; ## [20] &quot;e&quot; &quot;k&quot; &quot;s&quot; &quot;p&quot; &quot;å&quot; &quot;l&quot; &quot;a&quot; &quot;n&quot; &quot;d&quot; &quot;e&quot; &quot;v&quot; &quot;e&quot; &quot;j&quot; &quot;e&quot; &quot;n&quot; str_extract_all(danish_sentence, &quot;[a-zA-Z]&quot;) ## [[1]] ## [1] &quot;S&quot; &quot;m&quot; &quot;d&quot; &quot;t&quot; &quot;e&quot; &quot;h&quot; &quot;a&quot; &quot;n&quot; &quot;e&quot; &quot;n&quot; &quot;g&quot; &quot;a&quot; &quot;m&quot; &quot;m&quot; &quot;e&quot; &quot;l&quot; &quot;h&quot; &quot;e&quot; &quot;k&quot; ## [20] &quot;s&quot; &quot;p&quot; &quot;l&quot; &quot;a&quot; &quot;n&quot; &quot;d&quot; &quot;e&quot; &quot;v&quot; &quot;e&quot; &quot;j&quot; &quot;e&quot; &quot;n&quot; 2.4.2 Allow for hyphenated words in tokenize_words() Let’s make “fir-tree” a single word token. One way to do this is to split texts on white space, and dropping punctuations. str_split(&quot;This isn&#39;t a sentence with fir-tree.&quot;, &quot;[:space:]&quot;) ## [[1]] ## [1] &quot;This&quot; &quot;isn&#39;t&quot; &quot;a&quot; &quot;sentence&quot; &quot;with&quot; &quot;fir-tree.&quot; str_split(&quot;This isn&#39;t a sentence with fir-tree.&quot;, &quot;[:space:]&quot;) %&gt;% map(~ str_remove_all(.x, &quot;^[:punct:]+|[:punct:]+$&quot;)) ## [[1]] ## [1] &quot;This&quot; &quot;isn&#39;t&quot; &quot;a&quot; &quot;sentence&quot; &quot;with&quot; &quot;fir-tree&quot; Another way is to extract the hyphenated word. str_extract_all( string = &quot;This isn&#39;t a sentence with fir-tree.&quot;, pattern = &quot;[:alpha:]+-[:alpha:]+&quot; ) ## [[1]] ## [1] &quot;fir-tree&quot; use ? quantifier in regex, to optionally match pattern. str_extract_all( string = &quot;This isn&#39;t a sentence with fir-tree.&quot;, pattern = &quot;[:alpha:]+-?[:alpha:]+&quot; ) ## [[1]] ## [1] &quot;This&quot; &quot;isn&quot; &quot;sentence&quot; &quot;with&quot; &quot;fir-tree&quot; include ' in the [:alpha:] class str_extract_all( string = &quot;This isn&#39;t a sentence with fir-tree.&quot;, pattern = &quot;[[:alpha:]&#39;]+-?[[:alpha:]&#39;]+&quot; ) ## [[1]] ## [1] &quot;This&quot; &quot;isn&#39;t&quot; &quot;sentence&quot; &quot;with&quot; &quot;fir-tree&quot; The letter “a” is missing, because the regex so far assumes at least 2 characters. Get around that, by using | to set up a match for one or more [:alpha:] str_extract_all( string = &quot;This isn&#39;t a sentence with fir-tree.&quot;, pattern = &quot;[[:alpha:]&#39;]+-?[[:alpha:]&#39;]+|[:alpha:]{1}&quot; ) ## [[1]] ## [1] &quot;This&quot; &quot;isn&#39;t&quot; &quot;a&quot; &quot;sentence&quot; &quot;with&quot; &quot;fir-tree&quot; 2.4.3 Character n-gram tokenizer tokenize_character_ngram &lt;- function(x, n) { ngram_loc &lt;- str_locate_all(x, paste0(&quot;(?=(\\\\w{&quot;, n, &quot;}))&quot;)) map2(ngram_loc, x, ~str_sub(.y, .x[, 1], .x[, 1] + n - 1)) } tokenize_character_ngram(the_fir_tree[1:3], n = 3) ## [[1]] ## [1] &quot;Far&quot; &quot;dow&quot; &quot;own&quot; &quot;the&quot; &quot;for&quot; &quot;ore&quot; &quot;res&quot; &quot;est&quot; &quot;whe&quot; &quot;her&quot; &quot;ere&quot; &quot;the&quot; ## [13] &quot;war&quot; &quot;arm&quot; &quot;sun&quot; &quot;and&quot; &quot;the&quot; &quot;fre&quot; &quot;res&quot; &quot;esh&quot; &quot;air&quot; &quot;mad&quot; &quot;ade&quot; &quot;swe&quot; ## [25] &quot;wee&quot; &quot;eet&quot; ## ## [[2]] ## [1] &quot;res&quot; &quot;est&quot; &quot;sti&quot; &quot;tin&quot; &quot;ing&quot; &quot;pla&quot; &quot;lac&quot; &quot;ace&quot; &quot;gre&quot; &quot;rew&quot; &quot;pre&quot; &quot;ret&quot; ## [13] &quot;ett&quot; &quot;tty&quot; &quot;lit&quot; &quot;itt&quot; &quot;ttl&quot; &quot;tle&quot; &quot;fir&quot; &quot;tre&quot; &quot;ree&quot; &quot;and&quot; &quot;yet&quot; &quot;was&quot; ## [25] &quot;not&quot; &quot;hap&quot; &quot;app&quot; &quot;ppy&quot; ## ## [[3]] ## [1] &quot;wis&quot; &quot;ish&quot; &quot;she&quot; &quot;hed&quot; &quot;muc&quot; &quot;uch&quot; &quot;tal&quot; &quot;all&quot; &quot;lik&quot; &quot;ike&quot; &quot;its&quot; &quot;com&quot; ## [13] &quot;omp&quot; &quot;mpa&quot; &quot;pan&quot; &quot;ani&quot; &quot;nio&quot; &quot;ion&quot; &quot;ons&quot; &quot;the&quot; &quot;pin&quot; &quot;ine&quot; &quot;nes&quot; &quot;and&quot; ## [25] &quot;fir&quot; &quot;irs&quot; &quot;whi&quot; &quot;hic&quot; &quot;ich&quot; &quot;gre&quot; &quot;rew&quot; "],["meeting-videos-1.html", "2.5 Meeting Videos", " 2.5 Meeting Videos 2.5.1 Cohort 1 Meeting chat log 00:41:34 shamsuddeen: https://github.com/quanteda/spacyr "],["stop-words.html", "Chapter 3 Stop words", " Chapter 3 Stop words Learning objectives: Investigate what a stop word list is, Identify and use off-the-shelf stopwords Create your own stopwords "],["what-are-stop-words.html", "3.1 What are stop-words", " 3.1 What are stop-words Previous chapter discuss tokenization. It turns out not all words carry the same amount of information. So, we need to pre-process our data and remove those words with little or no information The process of converting data to something a computer can understand is referred to as pre-processing. One of the major forms of pre-processing is to filter out useless data. In natural language processing, useless words (data), are referred to as stop words. "],["why-do-we-remove-them.html", "3.2 Why do we remove them", " 3.2 Why do we remove them Stopwords take up space and valuable processing time. For this, we can remove them easily, it can be regarded as a dimensionality reduction of text data. Does it make sense in all NLP task to remove stopwords? "],["when-stopwords-removal-make-sense.html", "3.3 When stopwords removal make sense?", " 3.3 When stopwords removal make sense? Removal of stopwords depends on the task you are solving. There is no hard and fast rule on when to remove stop words. But we should remove stop words if our task is one of Language Classification, Spam Filtering, Caption Generation, Auto-Tag Generation, Sentiment analysis, or something that is related to text classification On the other hand, if our task is one of Machine Translation, Question-Answering problems, Text Summarization, Language Modeling, it’s better not to remove the stop words as they are a crucial part of these applications Less complex Model: Stop words do not carry meaning on their own, but only in the context of a sentence. If you use a model (a linear classifier, decision tree/forest) that is in principle incapable of leveraging the context, keeping the stop words cannot actually help. Complex Model : But, if you use more complex models (LSTM, Transformers) that can grasp the grammatical meaning of the stopwords, it does not make sense to remove them. Sentiment analysis task is sensitive to stop words (e.g., David is not happy vs (David, Happy)) So, what kind of stopwords to inlude depend on the task at hand? it’s better to keep these words and do some tests with and without them to see how it affects the model and you should never remove stop words without thinking about the impact of these words on the problem you are trying to solve. "],["using-off-the-shelf-stop-word-lists.html", "3.4 Using off-the-shelf stop word lists", " 3.4 Using off-the-shelf stop word lists A quick option for using stop words is to get a list that has already been created. There are many lits available, but not all lists are created equal Quanteda provides multilingual stopwords library(quanteda) stopwords::stopwords_getsources() ## [1] &quot;snowball&quot; &quot;stopwords-iso&quot; &quot;misc&quot; &quot;smart&quot; ## [5] &quot;marimo&quot; &quot;ancient&quot; &quot;nltk&quot; &quot;perseus&quot; Get languages supported by a stopwords stopwords::stopwords_getlanguages(&quot;snowball&quot;) ## [1] &quot;da&quot; &quot;de&quot; &quot;en&quot; &quot;es&quot; &quot;fi&quot; &quot;fr&quot; &quot;hu&quot; &quot;ir&quot; &quot;it&quot; &quot;nl&quot; &quot;no&quot; &quot;pt&quot; &quot;ro&quot; &quot;ru&quot; &quot;sv&quot; stopwords::stopwords_getlanguages(&quot;nltk&quot;) ## [1] &quot;ar&quot; &quot;az&quot; &quot;da&quot; &quot;nl&quot; &quot;en&quot; &quot;fi&quot; &quot;fr&quot; &quot;de&quot; &quot;el&quot; &quot;hu&quot; &quot;id&quot; &quot;it&quot; &quot;kk&quot; &quot;ne&quot; &quot;no&quot; ## [16] &quot;pt&quot; &quot;ro&quot; &quot;ru&quot; &quot;sl&quot; &quot;es&quot; &quot;sv&quot; &quot;tg&quot; &quot;tr&quot; stopwords::stopwords_getlanguages(&quot;stopwords-iso&quot;) ## [1] &quot;af&quot; &quot;ar&quot; &quot;hy&quot; &quot;eu&quot; &quot;bn&quot; &quot;br&quot; &quot;bg&quot; &quot;ca&quot; &quot;zh&quot; &quot;hr&quot; &quot;cs&quot; &quot;da&quot; &quot;nl&quot; &quot;en&quot; &quot;eo&quot; ## [16] &quot;et&quot; &quot;fi&quot; &quot;fr&quot; &quot;gl&quot; &quot;de&quot; &quot;el&quot; &quot;ha&quot; &quot;he&quot; &quot;hi&quot; &quot;hu&quot; &quot;id&quot; &quot;ga&quot; &quot;it&quot; &quot;ja&quot; &quot;ko&quot; ## [31] &quot;ku&quot; &quot;la&quot; &quot;lt&quot; &quot;lv&quot; &quot;ms&quot; &quot;mr&quot; &quot;no&quot; &quot;fa&quot; &quot;pl&quot; &quot;pt&quot; &quot;ro&quot; &quot;ru&quot; &quot;sk&quot; &quot;sl&quot; &quot;so&quot; ## [46] &quot;st&quot; &quot;es&quot; &quot;sw&quot; &quot;sv&quot; &quot;th&quot; &quot;tl&quot; &quot;tr&quot; &quot;uk&quot; &quot;ur&quot; &quot;vi&quot; &quot;yo&quot; &quot;zu&quot; Default stopword in Quanteeda is snowball. Why? length(stopwords::stopwords(source = &quot;smart&quot;)) ## [1] 571 length(stopwords::stopwords(source = &quot;snowball&quot;)) ## [1] 175 length(stopwords::stopwords(source = &quot;stopwords-iso&quot;)) ## [1] 1298 These stopwords do intersect Bt, words that appear in Snowball and ISO but not in the SMART list. setdiff(stopwords(source = &quot;snowball&quot;), stopwords(source = &quot;smart&quot;)) ## [1] &quot;she&#39;s&quot; &quot;he&#39;d&quot; &quot;she&#39;d&quot; &quot;he&#39;ll&quot; &quot;she&#39;ll&quot; &quot;shan&#39;t&quot; &quot;mustn&#39;t&quot; ## [8] &quot;when&#39;s&quot; &quot;why&#39;s&quot; &quot;how&#39;s&quot; "],["stop-word-removal-in-r.html", "3.5 Stop word removal in R", " 3.5 Stop word removal in R library(hcandersenr) library(tidyverse) library(tidytext) fir_tree &lt;- hca_fairytales() %&gt;% filter(book == &quot;The fir tree&quot;, language == &quot;English&quot;) fir_tree ## # A tibble: 253 × 3 ## text book language ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 &quot;Far down in the forest, where the warm sun and the fresh air… The … English ## 2 &quot;resting-place, grew a pretty little fir-tree; and yet it was… The … English ## 3 &quot;wished so much to be tall like its companions– the pines and… The … English ## 4 &quot;around it. The sun shone, and the soft air fluttered its lea… The … English ## 5 &quot;little peasant children passed by, prattling merrily, but th… The … English ## 6 &quot;them not. Sometimes the children would bring a large basket … The … English ## 7 &quot;strawberries, wreathed on a straw, and seat themselves near … The … English ## 8 &quot;say, \\&quot;Is it not a pretty little tree?\\&quot; which made it feel … The … English ## 9 &quot;before.&quot; The … English ## 10 &quot;And yet all this while the tree grew a notch or joint taller… The … English ## # … with 243 more rows tidy_fir_tree &lt;- fir_tree %&gt;% unnest_tokens(word, text) tidy_fir_tree ## # A tibble: 3,288 × 3 ## book language word ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 The fir tree English far ## 2 The fir tree English down ## 3 The fir tree English in ## 4 The fir tree English the ## 5 The fir tree English forest ## 6 The fir tree English where ## 7 The fir tree English the ## 8 The fir tree English warm ## 9 The fir tree English sun ## 10 The fir tree English and ## # … with 3,278 more rows Stopwords that return vector tidy_fir_tree %&gt;% filter(!(word %in% stopwords(source = &quot;snowball&quot;))) ## # A tibble: 1,547 × 3 ## book language word ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 The fir tree English far ## 2 The fir tree English forest ## 3 The fir tree English warm ## 4 The fir tree English sun ## 5 The fir tree English fresh ## 6 The fir tree English air ## 7 The fir tree English made ## 8 The fir tree English sweet ## 9 The fir tree English resting ## 10 The fir tree English place ## # … with 1,537 more rows If we use the get_stopwords() function from tidytext instead, then we can use the anti_join() function. tidy_fir_tree %&gt;% anti_join(get_stopwords(source = &quot;snowball&quot;)) ## Joining, by = &quot;word&quot; ## # A tibble: 1,547 × 3 ## book language word ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 The fir tree English far ## 2 The fir tree English forest ## 3 The fir tree English warm ## 4 The fir tree English sun ## 5 The fir tree English fresh ## 6 The fir tree English air ## 7 The fir tree English made ## 8 The fir tree English sweet ## 9 The fir tree English resting ## 10 The fir tree English place ## # … with 1,537 more rows It is perfectly acceptable to start with a premade word list and remove or append additional words according to your particular use case. adding to the built-in stopword list toks &lt;- tokens(&quot;The judge will sentence Mr. Adams to nine years in prison&quot;, remove_punct = TRUE) toks ## Tokens consisting of 1 document. ## text1 : ## [1] &quot;The&quot; &quot;judge&quot; &quot;will&quot; &quot;sentence&quot; &quot;Mr&quot; &quot;Adams&quot; ## [7] &quot;to&quot; &quot;nine&quot; &quot;years&quot; &quot;in&quot; &quot;prison&quot; adding : “will”, “mr”, “nine” tokens_remove(toks, c(stopwords(&quot;english&quot;), &quot;will&quot;, &quot;mr&quot;, &quot;nine&quot;)) ## Tokens consisting of 1 document. ## text1 : ## [1] &quot;judge&quot; &quot;sentence&quot; &quot;Adams&quot; &quot;years&quot; &quot;prison&quot; "],["creating-your-own-stop-words-list.html", "3.6 Creating your own stop words list", " 3.6 Creating your own stop words list We can create our own stopwords using monolingual corpus Monolingual corpus Vs Multilingual Using Fir-Tree dataset, let’s take the words and rank them by their count or frequency. tidy_fir_tree %&gt;% count(word, sort = TRUE) %&gt;% slice(1:120) %&gt;% mutate(word = paste0(row_number(), &quot;: &quot;, word)) %&gt;% pull(word) %&gt;% columnize() p { font-family:'Roboto Condensed';font-size:12pt;line-height:12.5pt;padding:0;margin:0} 1: the 2: and 3: tree 4: it 5: a 6: in 7: of 8: to 9: i 10: was 11: they 12: fir 13: were 14: all 15: with 16: but 17: on 18: then 19: had 20: is 21: at 22: little 23: so 24: not 25: said 26: what 27: as 28: that 29: he 30: you 31: its 32: out 33: be 34: them 35: this 36: branches 37: came 38: for 39: now 40: one 41: story 42: would 43: forest 44: have 45: how 46: know 47: thought 48: mice 49: trees 50: we 51: been 52: down 53: oh 54: very 55: when 56: where 57: who 58: children 59: dumpty 60: humpty 61: or 62: shall 63: there 64: while 65: will 66: after 67: by 68: come 69: happy 70: my 71: old 72: only 73: their 74: which 75: again 76: am 77: are 78: beautiful 79: evening 80: him 81: like 82: me 83: more 84: about 85: christmas 86: do 87: fell 88: fresh 89: from 90: here 91: last 92: much 93: no 94: princess 95: tall 96: young 97: asked 98: can 99: could 100: cried 101: going 102: grew 103: if 104: large 105: looked 106: made 107: many 108: seen 109: stairs 110: think 111: too 112: up 113: yes 114: air 115: also 116: away 117: birds 118: corner 119: cut 120: did this has “three” as stopwords which does not make sense. How can we solve this issue? -Use large corpus will give a descent one Use multiple genre subject-specific corpus. Stopwords can then be intersection of stopwords in each corpus. Leveraging languages speakers and domain expert yo &lt;- read_csv(&quot;https://raw.githubusercontent.com/Niger-Volta-LTI/yoruba-text/master/Alabi_YorubaTwi_Embedding/alakowe.txt&quot;, col_names = FALSE) %&gt;% select(1) %&gt;% rename(text = 1) # Mean Frequency yo1_stop_mean &lt;- yo %&gt;% unnest_tokens(word, text) %&gt;% count(word, sort = TRUE) %&gt;% mutate(mean_frequency = n/nrow(yo)) %&gt;% mutate(rank_freq = row_number()) %&gt;% filter(n &gt;2) %&gt;% select(word) %&gt;% slice(1:20) yo1_stop_mean ## # A tibble: 20 × 1 ## word ## &lt;chr&gt; ## 1 ni ## 2 tí ## 3 àwọn ## 4 ní ## 5 wọ́n ## 6 ó ## 7 ti ## 8 náà ## 9 tó ## 10 a ## 11 ṣe ## 12 pé ## 13 yìí ## 14 sí ## 15 bá ## 16 ń ## 17 àti ## 18 mo ## 19 máa ## 20 fún yo_two &lt;- read_csv(&quot;https://raw.githubusercontent.com/Niger-Volta-LTI/yoruba-text/master/Alabi_YorubaTwi_Embedding/edeyorubarewa.txt&quot;, col_names = FALSE)%&gt;% select(1) %&gt;% rename(text = 1) # Mean Frequency yo_two_stop &lt;- yo_two %&gt;% unnest_tokens(word, text) %&gt;% count(word, sort = TRUE) %&gt;% mutate(mean_frequency = n/nrow(yo_two)) %&gt;% mutate(rank_freq = row_number()) %&gt;% filter(n &gt;2) %&gt;% select(word) %&gt;% slice(1:20) yo_two_stop ## # A tibble: 20 × 1 ## word ## &lt;chr&gt; ## 1 tí ## 2 ni ## 3 ó ## 4 ń ## 5 àwọn ## 6 ní ## 7 yìí ## 8 ẹ ## 9 kí ## 10 fún ## 11 ọba ## 12 a ## 13 pé ## 14 sí ## 15 bá ## 16 ṣe ## 17 bí ## 18 tó ## 19 mo ## 20 ti intersect(yo_two_stop,yo1_stop_mean) ## # A tibble: 16 × 1 ## word ## &lt;chr&gt; ## 1 tí ## 2 ni ## 3 ó ## 4 ń ## 5 àwọn ## 6 ní ## 7 yìí ## 8 fún ## 9 a ## 10 pé ## 11 sí ## 12 bá ## 13 ṣe ## 14 tó ## 15 mo ## 16 ti "],["stopwords-for-pidgin.html", "3.7 Stopwords for Pidgin", " 3.7 Stopwords for Pidgin Pidgin is an English-based creole language spoken as a lingua franca Nigerian Pidgin (Naija) is an English-based creole language spoken across Nigeria For example, “Dis food sweet well, well/Dis food sweet no be smal” in Pidgin English translates to “This meal is delicious” dic &lt;- enframe(read_rds(&quot;data/wiktionary_words.rds&quot;)) %&gt;% rename(words = value) %&gt;% mutate(word = str_extract_all(words, boundary(&quot;word&quot;)) ) %&gt;% unnest(word) %&gt;% select(word) %&gt;% arrange(word) Piding &lt;- read_tsv(&quot;https://raw.githubusercontent.com/keleog/PidginUNMT/master/corpus/monolingual/pidgin_corpus.txt&quot;, col_names = FALSE) %&gt;% rename(text = 1) Piding %&gt;% unnest_tokens(word, text) %&gt;% count(word, sort = TRUE) %&gt;% mutate(mean_frequency = n/nrow(Piding)) %&gt;% mutate(rank_freq = row_number()) %&gt;% filter(n &gt;2) %&gt;% select(word) %&gt;% head(10) ## # A tibble: 10 × 1 ## word ## &lt;chr&gt; ## 1 for ## 2 dey ## 3 say ## 4 wey ## 5 di ## 6 the ## 7 and ## 8 dem ## 9 to ## 10 of Piding %&gt;% unnest_tokens(word, text) %&gt;% count(word, sort = TRUE) %&gt;% mutate(mean_frequency = n/nrow(Piding)) %&gt;% mutate(rank_freq = row_number()) %&gt;% filter(n &gt;2) %&gt;% select(word)%&gt;% anti_join(dic) %&gt;% head(10) ## Joining, by = &quot;word&quot; ## # A tibble: 10 × 1 ## word ## &lt;chr&gt; ## 1 pipo ## 2 nigeria ## 3 tok ## 4 wetin ## 5 buhari ## 6 goment ## 7 comot ## 8 pikin ## 9 pesin ## 10 apc "],["how-many-words-do-we-include-in-our-stop-word-list.html", "3.8 How many words do we include in our stop word list?", " 3.8 How many words do we include in our stop word list? It can be difficult to determine a priori how many different “meaningless” words appear in a corpus. But, Start with a low number like 20 and increase by 10 words until you get to words that are not appropriate as stop words for your analytical purpose. There’s no universal stop words list because a word can be empty of meaning depending on the corpus you are using or on the problem you are analysing "],["all-stop-word-lists-are-context-specific.html", "3.9 All stop word lists are context-specific", " 3.9 All stop word lists are context-specific Context is important in text modeling, so it is important to ensure that the stop word lexicon you use reflects the word space that you are planning on using it in On the other hand, sometimes you will have to add in words yourself, depending on the domain. "],["problem-with-off-the-shelf-stopwords.html", "3.10 Problem with off-the-shelf stopwords", " 3.10 Problem with off-the-shelf stopwords Most premade stop word lists assume that all the words are spelled correctly. Handling misspellings when using premade lists can be done by manually adding common misspellings Manually and automatically (Chinese has both and non has been accepted as a standard). "],["meeting-videos-2.html", "3.11 Meeting Videos", " 3.11 Meeting Videos 3.11.1 Cohort 1 Meeting chat log 00:14:48 Hong: Hi Sham, sorry if I pronounce your name wrong.. Could you please share the link of a copy of the text book? thank you.. the screen is a little bit small to read on my small laptop screen. 00:15:16 Justin Dollman: Ok! 00:15:30 Justin Dollman: https://smltar.com/ 00:15:58 Hong: Thank you :) 00:18:43 Hong: thank you 00:28:35 Justin Dollman: https://r4ds.github.io/bookclub-smltar/ 00:43:25 Justin Dollman: “How do I get this grass (weed!) out of my flower pot!! #brooklyngardenproblems” 00:43:44 Justin Dollman: Classification about drugs: 99% certainty 00:43:50 Layla Bouzoubaa: bahahahahah 00:43:58 Layla Bouzoubaa: Story of my life "],["stemming.html", "Chapter 4 Stemming", " Chapter 4 Stemming Learning objectives: (these are optional but helpful) "],["slide-1.html", "4.1 Slide 1", " 4.1 Slide 1 Add slides as sections (marked with ##). Please give code chunks unique names (such as “01-something” for a block in chapter 1). This makes debugging much easier. "],["meeting-videos-3.html", "4.2 Meeting Videos", " 4.2 Meeting Videos 4.2.1 Cohort 1 Meeting chat log 00:58:09 shamsuddeen: http://talnarchives.atala.org/ateliers/2016/CLTW/2.pdf 00:58:20 shamsuddeen: https://github.com/ancatmara/early-irish-lemmatizer "],["word-embeddings.html", "Chapter 5 Word Embeddings", " Chapter 5 Word Embeddings Learning objectives: (these are optional but helpful) "],["slide-1-1.html", "5.1 Slide 1", " 5.1 Slide 1 Add slides as sections (marked with ##). Please give code chunks unique names (such as “01-something” for a block in chapter 1). This makes debugging much easier. "],["meeting-videos-4.html", "5.2 Meeting Videos", " 5.2 Meeting Videos 5.2.1 Cohort 1 "],["regression.html", "Chapter 6 Regression", " Chapter 6 Regression Learning objectives: (these are optional but helpful) "],["slide-1-2.html", "6.1 Slide 1", " 6.1 Slide 1 Add slides as sections (marked with ##). Please give code chunks unique names (such as “01-something” for a block in chapter 1). This makes debugging much easier. "],["meeting-videos-5.html", "6.2 Meeting Videos", " 6.2 Meeting Videos 6.2.1 Cohort 1 "],["classification.html", "Chapter 7 Classification", " Chapter 7 Classification Learning objectives: (these are optional but helpful) "],["slide-1-3.html", "7.1 Slide 1", " 7.1 Slide 1 Add slides as sections (marked with ##). Please give code chunks unique names (such as “01-something” for a block in chapter 1). This makes debugging much easier. "],["meeting-videos-6.html", "7.2 Meeting Videos", " 7.2 Meeting Videos 7.2.1 Cohort 1 Meeting chat log 00:53:20 Justin Dollman: just fyi, it’s grid_latin_hypercube() that will use a latin hypercube 00:53:30 Justin Dollman: grid_regular does evenly spaced stuff 00:53:49 Jiwan Heo: doesn&#39;t grid_regular call hypercube somewhere? 00:54:14 Justin Dollman: i don’t think so… 00:54:42 Justin Dollman: but if i’m wrong i’d like to know 😅 00:55:03 Jiwan Heo: yea I remember incorrectly answering a slack question, and Max corrected me one time lol "],["dense-neural-networks.html", "Chapter 8 Dense neural networks", " Chapter 8 Dense neural networks Learning objectives: (these are optional but helpful) "],["slide-1-4.html", "8.1 Slide 1", " 8.1 Slide 1 Add slides as sections (marked with ##). Please give code chunks unique names (such as “01-something” for a block in chapter 1). This makes debugging much easier. "],["meeting-videos-7.html", "8.2 Meeting Videos", " 8.2 Meeting Videos 8.2.1 Cohort 1 Meeting chat log LOG "],["long-short-term-memory-lstm-networks.html", "Chapter 9 Long short-term memory (LSTM) networks", " Chapter 9 Long short-term memory (LSTM) networks Learning objectives: (these are optional but helpful) "],["slide-1-5.html", "9.1 Slide 1", " 9.1 Slide 1 Add slides as sections (marked with ##). Please give code chunks unique names (such as “01-something” for a block in chapter 1). This makes debugging much easier. "],["meeting-videos-8.html", "9.2 Meeting Videos", " 9.2 Meeting Videos 9.2.1 Cohort 1 Meeting chat log LOG "],["convolutional-neural-networks.html", "Chapter 10 Convolutional neural networks", " Chapter 10 Convolutional neural networks Learning objectives: (these are optional but helpful) "],["slide-1-6.html", "10.1 Slide 1", " 10.1 Slide 1 Add slides as sections (marked with ##). Please give code chunks unique names (such as “01-something” for a block in chapter 1). This makes debugging much easier. "],["meeting-videos-9.html", "10.2 Meeting Videos", " 10.2 Meeting Videos 10.2.1 Cohort 1 Meeting chat log LOG "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
